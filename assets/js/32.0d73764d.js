(window.webpackJsonp=window.webpackJsonp||[]).push([[32],{316:function(t,a,s){"use strict";s.r(a);var v=s(10),_=Object(v.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"机器学习课程-李宏毅"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#机器学习课程-李宏毅"}},[t._v("#")]),t._v(" 机器学习课程-李宏毅")]),t._v(" "),a("p",[a("a",{attrs:{href:"https://www.bilibili.com/video/BV1Wv411h7kN?spm_id_from=333.788.videopod.episodes&vd_source=ed6f8916d713e6a2c8059c684a250ee6",target:"_blank",rel:"noopener noreferrer"}},[t._v("B站课程链接"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("a",{attrs:{href:"https://www.youtube.com/watch?v=7XZR0-4uS5s&list=PLJV_el3uVTsPM2mM-OQzJXziCGJa8nJL8",target:"_blank",rel:"noopener noreferrer"}},[t._v("youtube课程地址"),a("OutboundLink")],1)]),t._v(" "),a("h2",{attrs:{id:"机器学习的基本概念"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#机器学习的基本概念"}},[t._v("#")]),t._v(" 机器学习的基本概念")]),t._v(" "),a("p",[t._v("1-5 Supervised Learning")]),t._v(" "),a("p",[t._v("6 生成式对抗网络  Generative Adversarial Network")]),t._v(" "),a("p",[t._v("7 自监督学习 self-supervised Learning")]),t._v(" "),a("p",[t._v("BERT  基础模型")]),t._v(" "),a("p",[t._v("8 异常检测 Anomaly Detection")]),t._v(" "),a("p",[t._v("9 可解释性AI  Explainable AI")]),t._v(" "),a("p",[t._v("10 模型攻击 Model Attack")]),t._v(" "),a("p",[t._v("11 领域适应 Domain Adaptation")]),t._v(" "),a("p",[t._v("12 强化学习 Reinforcement Learning")]),t._v(" "),a("p",[t._v("13 模型压缩 Network Compression")]),t._v(" "),a("p",[t._v("14 终身机器学习 Life-long Learning")]),t._v(" "),a("p",[t._v("15 元学习-学习如何学习 Meta learning")]),t._v(" "),a("p",[t._v("google colab")]),t._v(" "),a("p",[t._v("浅谈机器学习原理")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251215221112172.png",alt:"image-20251215221112172"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251215222251713.png",alt:"image-20251215222251713"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251215223140651.png",alt:"image-20251215223140651"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251215223742080.png",alt:"image-20251215223742080"}})]),t._v(" "),a("p",[t._v("VC-dimension 描述一个模型复杂程度的指标")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251215224734087.png",alt:"image-20251215224734087"}})]),t._v(" "),a("p",[a("a",{attrs:{href:"https://www.youtube.com/watch?v=xQXh3fSvD1A&list=PLJV_el3uVTsPM2mM-OQzJXziCGJa8nJL8&index=3",target:"_blank",rel:"noopener noreferrer"}},[t._v("视频进度"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("structure learning 让机器产生有结构的输出")]),t._v(" "),a("p",[t._v("机器学习公式推导比较难的一步是计算梯度， 而pytorch框架可以自动计算梯度。")]),t._v(" "),a("h2",{attrs:{id:"机器学习任务攻略"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#机器学习任务攻略"}},[t._v("#")]),t._v(" 机器学习任务攻略")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224094027420.png",alt:"image-20251224094027420"}}),t._v(" "),a("p",[t._v("当LOSS比较大的时候，可能是模型偏差，也可能是优化没做好，怎么判断呢？")]),t._v(" "),a("ul",[a("li",[t._v("使用容易优化的浅层的网络，或者其他的模型")]),t._v(" "),a("li",[t._v("如果更深的模型在训练数据上没有获得更小的loss，那就是优化没做好")])]),t._v(" "),a("blockquote",[a("p",[t._v("深层网络一定可以做到浅层网络做到的事情，比如50层的网络，前20层可以和20层的网络做相同的事情，后30层什么也不做，那50层的网络效果和20层的网络就是一样的。")])]),t._v(" "),a("h3",{attrs:{id:"过拟合问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#过拟合问题"}},[t._v("#")]),t._v(" 过拟合问题")]),t._v(" "),a("p",[t._v("过拟合问题解决方案")]),t._v(" "),a("ul",[a("li",[t._v("增加训练数据")]),t._v(" "),a("li",[t._v("数据增强：比如图像领域机器学习，可以把图片左右翻转，部分放大")]),t._v(" "),a("li",[t._v("给模型一些限制：少的参数，共用参数")]),t._v(" "),a("li",[t._v("更少的要素")]),t._v(" "),a("li",[t._v("正则化")]),t._v(" "),a("li",[t._v("Early stopping：在验证集性能达到最优时，提前停止训练")]),t._v(" "),a("li",[t._v("Dropout")])]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224102254368.png",alt:"image-20251224102254368"}}),t._v(" "),a("p",[t._v("模型太简单会有模型偏差，模型过于复杂会过拟合。")]),t._v(" "),a("h3",{attrs:{id:"模型泛化能力"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#模型泛化能力"}},[t._v("#")]),t._v(" 模型泛化能力")]),t._v(" "),a("p",[t._v("公共测试集：比赛开发阶段用于评估模型性能的测试数据")]),t._v(" "),a("p",[t._v("私有测试集：比赛最终阶段（盲测阶段）使用的隐藏测试数据")]),t._v(" "),a("p",[t._v("因此需要把训练数据集分为两部分，一部分用于训练，一部分用于验证。选择验证集上LOSS最小的模型就好。")]),t._v(" "),a("h3",{attrs:{id:"n-折交叉验证"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#n-折交叉验证"}},[t._v("#")]),t._v(" n 折交叉验证")]),t._v(" "),a("p",[t._v("用于评估模型的泛化能力")]),t._v(" "),a("p",[t._v("将"),a("strong",[t._v("整个数据集")]),t._v("随机且均匀地划分为 "),a("code",[t._v("n")]),t._v(" 个互斥的子集（称为 "),a("strong",[t._v("fold")]),t._v("，即 “折”），然后重复 "),a("code",[t._v("n")]),t._v(" 次训练和验证：")]),t._v(" "),a("ol",[a("li",[t._v("每次选取 "),a("strong",[t._v("1 个 fold 作为验证集")]),t._v("，剩下的 "),a("strong",[t._v("n-1 个 fold 作为训练集")]),t._v("；")]),t._v(" "),a("li",[t._v("用训练集训练模型，用验证集计算性能指标（如准确率、Loss）；")]),t._v(" "),a("li",[t._v("完成 "),a("code",[t._v("n")]),t._v(" 次循环后，取 "),a("strong",[t._v("n 次验证指标的平均值")]),t._v("，作为模型的最终评估结果。")])]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224104702357.png",alt:"image-20251224104702357"}}),t._v(" "),a("p",[t._v("mismatch：分布偏移或模型不匹配。模型的训练逻辑是架设训练数据和测试数据服从相同的概率分布，一旦这个架设不成立，模型学到的规律就无法适配测试数据。")]),t._v(" "),a("h2",{attrs:{id:"模型优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#模型优化"}},[t._v("#")]),t._v(" 模型优化")]),t._v(" "),a("h3",{attrs:{id:"鞍点、局部最小、局部最大"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#鞍点、局部最小、局部最大"}},[t._v("#")]),t._v(" 鞍点、局部最小、局部最大")]),t._v(" "),a("p",[t._v("当梯度为0时，不一定是局部最小值，也有可能是鞍点。")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224105658657.png",alt:"image-20251224105658657"}}),t._v(" "),a("p",[t._v("用泰勒展开计算：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224110411782.png",alt:"image-20251224110411782"}}),t._v(" "),a("p",[t._v("H是海森矩阵，多元函数的的二阶偏导数方阵，可以使用海森矩阵正负定性判断极值点。")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224110957649.png",alt:"image-20251224110957649"}}),t._v(" "),a("p",[t._v("正定矩阵：实对称矩阵所有特征值大于0，称为正定矩阵。")]),t._v(" "),a("blockquote",[a("p",[t._v("海森矩阵的特征值有正有负，表示该点是鞍点。")]),t._v(" "),a("p",[t._v("沿着"),a("code",[t._v("负特征值")]),t._v("对应的"),a("code",[t._v("特征向量")]),t._v("方向更新参数，可以降低LOSS。")])]),t._v(" "),a("p",[t._v("二维的局部最小值在三维看来可能是一个鞍点，低维度看起来无路可走，在高维度其实是有路的。")]),t._v(" "),a("h3",{attrs:{id:"批次和动量"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#批次和动量"}},[t._v("#")]),t._v(" 批次和动量")]),t._v(" "),a("p",[t._v("为什么要用batch?")]),t._v(" "),a("p",[t._v("当并行计算时，较大的batch size计算梯度时不一定比小的batch size 花的时间长。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224142331956.png",alt:"image-20251224142331956"}})]),t._v(" "),a("p",[t._v("小的batch size 在优化和测试集上效果更好。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224143235938.png",alt:"image-20251224143235938"}})]),t._v(" "),a("p",[a("strong",[t._v("动量Momentum")])]),t._v(" "),a("p",[t._v("梯度下降法：每次朝着梯度的反方向更新一步。")]),t._v(" "),a("p",[t._v("然而，在物理世界中，小球沿着Loss函数滚落，由于带有速度，不会在局部最小值停下。")]),t._v(" "),a("blockquote",[a("p",[t._v("动量只是类比物理世界中物体运动惯性的特点。")])]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224143520685.png",alt:"image-20251224143520685"}}),t._v(" "),a("p",[t._v("考虑动量的计算：每一步的变化量=上一步的变化量-当前步的梯度")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224144713982.png",alt:"image-20251224144713982"}}),t._v(" "),a("blockquote",[a("p",[t._v("引入动量有什么好处？")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("加速收敛速度")])]),t._v(" "),a("li",[a("strong",[t._v("抑制参数更新震荡")])]),t._v(" "),a("li",[a("strong",[t._v("突破局部极小值或鞍点")])])])]),t._v(" "),a("h3",{attrs:{id:"自动调整学习率"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#自动调整学习率"}},[t._v("#")]),t._v(" 自动调整学习率")]),t._v(" "),a("p",[t._v("当Loss不再减小的时候，并不意味着梯度一定很小，有可能在Minima附近震荡。")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224150109081.png",alt:"image-20251224150109081"}}),t._v(" "),a("blockquote",[a("p",[t._v("为什么需要自适应学习率？")]),t._v(" "),a("p",[t._v("固定学习率无法适配复杂误差曲面的不同区域，容易导致收敛慢、震荡或不收敛。")]),t._v(" "),a("p",[a("strong",[t._v("哪里难走，就放慢脚步；哪里好走，就加快速度")])])]),t._v(" "),a("h4",{attrs:{id:"典型的自适应算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#典型的自适应算法"}},[t._v("#")]),t._v(" 典型的自适应算法")]),t._v(" "),a("p",[t._v("Adagrad算法：累积参数的历史梯度平方和")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224152651590.png",alt:"image-20251224152651590"}}),t._v(" "),a("p",[t._v("RMSprop：指数移动平均累积梯度平方和")]),t._v(" "),a("blockquote",[a("p",[t._v("衡量近期梯度的波动幅度，远期的梯度影响会指数级衰减。")]),t._v(" "),a("p",[t._v("衰减系数α通常取0.9")])]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224153424585.png",alt:"image-20251224153424585"}}),t._v(" "),a("p",[a("strong",[t._v("Adam：结合动量 + RMSprop 的思想")])]),t._v(" "),a("h4",{attrs:{id:"学习率调度"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#学习率调度"}},[t._v("#")]),t._v(" 学习率调度")]),t._v(" "),a("p",[t._v("Learning Rate Decay：学习率衰减")]),t._v(" "),a("p",[t._v("初期学习率较大，末期学习率较小。")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224155358715.png",alt:"image-20251224155358715"}}),t._v(" "),a("p",[t._v("Warm Up：学习率先变大、后变小")]),t._v(" "),a("blockquote",[a("p",[t._v("在很多知名的网络都有使用。")])]),t._v(" "),a("h3",{attrs:{id:"不同的损失函数loss"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#不同的损失函数loss"}},[t._v("#")]),t._v(" 不同的损失函数Loss")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224162208465.png",alt:"image-20251224162208465"}})]),t._v(" "),a("p",[t._v("交叉熵最小等价于最大似然")]),t._v(" "),a("p",[a("strong",[t._v("交叉熵通常用于分类任务")]),t._v("。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224163754523.png",alt:"image-20251224163754523"}})]),t._v(" "),a("h2",{attrs:{id:"名词术语"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#名词术语"}},[t._v("#")]),t._v(" 名词术语")]),t._v(" "),a("p",[t._v("Model Bias：模型偏差，衡量模拟拟合能力不足的指标")]),t._v(" "),a("p",[t._v("piecewise linear ：分段线性")]),t._v(" "),a("p",[t._v("Loss：损失函数 means how good a et of values is .")]),t._v(" "),a("p",[t._v("epoch：计算梯度下降时，当所有的batch都计算一遍，作为一个epoch")]),t._v(" "),a("p",[t._v("update：每一次计算batch，更新参数，叫做update")]),t._v(" "),a("p",[t._v("hyperparameter： 超参数，机器学习中非机器自动计算的，人为设定的参数，比如batchsize，学习率等等")]),t._v(" "),a("p",[t._v("sigmoid函数")]),t._v(" "),a("p",[t._v("ReLU函数(Rectified Linear Unit)：修正线性单元")]),t._v(" "),a("p",[t._v("Nerual Network：神经网络")]),t._v(" "),a("p",[t._v("Overfitting：过拟合，在训练资料上变好，在测试资料上变差的情况")]),t._v(" "),a("p",[t._v("benchmark corpora：基准语料库，是自然语言处理（NLP）领域中，用于评估模型性能、对比不同算法效果的标准化数据集。")]),t._v(" "),a("p",[t._v("critical point ：临界点")]),t._v(" "),a("p",[t._v("error surface：误差曲面，损失函数关于参数的多维曲面。")])])}),[],!1,null,null,null);a.default=_.exports}}]);