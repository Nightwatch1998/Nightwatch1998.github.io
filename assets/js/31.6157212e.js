(window.webpackJsonp=window.webpackJsonp||[]).push([[31],{317:function(t,a,s){"use strict";s.r(a);var v=s(10),_=Object(v.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"机器学习入门-鱼书"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#机器学习入门-鱼书"}},[t._v("#")]),t._v(" 机器学习入门-鱼书")]),t._v(" "),a("p",[t._v("本文根据《深度学习入门：基于Python的理论与实现》一书总结，这本书非常适合深度学习入门学习，0基础也可以看懂。")]),t._v(" "),a("h2",{attrs:{id:"神经网络"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#神经网络"}},[t._v("#")]),t._v(" 神经网络")]),t._v(" "),a("h3",{attrs:{id:"mnist数据集"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mnist数据集"}},[t._v("#")]),t._v(" MNIST数据集")]),t._v(" "),a("p",[t._v("MNIST数据集是由0-9的数字图像构成的，训练图像有6万张，测试图像有1万张。一般的使用方法是，先用训练图像进行学习，再用学习到的模型度量能在多大程度上对测试图像进行正确的分类。")]),t._v(" "),a("p",[t._v("MNIST图像数据是28像素×28像素的灰度图像（1通道），各个像素取值在0-255之间，每个图像数据都有7、2、1等数字标签。")]),t._v(" "),a("p",[t._v("MNIST数据集用于手写数字识别，可以说是Deep Learning的Hello World!")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251111145423048.png",alt:"image-20251111145423048"}})]),t._v(" "),a("h2",{attrs:{id:"神经网络的学习"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#神经网络的学习"}},[t._v("#")]),t._v(" 神经网络的学习")]),t._v(" "),a("h3",{attrs:{id:"基本概念"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#基本概念"}},[t._v("#")]),t._v(" 基本概念")]),t._v(" "),a("p",[t._v("学习的目的是以损失函数为基础，找出能使它的值达到最小的权重参数。实际的神经网络中，参数的数量成千上万，在层数更深的深度学习中，参数的数量可能上亿，想要人工决定这些参数是不可能的，需要用数据来决定。")]),t._v(" "),a("p",[t._v("计算机视觉特征提取算法：")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("SIFT（尺度不变特征变换）")]),t._v("：主打 “尺度 + 旋转不变性”，能在不同大小、旋转角度的图像中找到相同特征，抗光照、噪声能力强，是经典的通用特征提取算法。")]),t._v(" "),a("li",[a("strong",[t._v("SURF（加速稳健特征）")]),t._v("：SIFT 的优化版，通过近似计算提升速度，同时保留了尺度和旋转不变性，适合对实时性有要求的场景。")]),t._v(" "),a("li",[a("strong",[t._v("HOG（方向梯度直方图）")]),t._v("：专注于 “形状与边缘” 特征，通过统计图像局部区域的梯度方向分布来描述目标轮廓，尤其适合行人检测等刚性目标识别。")])]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251111152257919.png",alt:"image-20251111152257919"}}),t._v(" "),a("p",[t._v("机器学习中，一般将数据分为训练数据和测试数据。首先使用训练数据进行学习，寻找最优的参数；然后使用测试数据评价训练的到的模型的实际泛化能力。训练数据也称为"),a("strong",[t._v("监督数据")]),t._v("。")]),t._v(" "),a("p",[t._v("仅用一个数据集去学习和评价参数，是无法进行正确评价的。这会导致可以顺利处理某个数据集，但不能处理其他数据集。只对一个数据集过度拟合的状态称为"),a("strong",[t._v("过拟合")]),t._v("。")]),t._v(" "),a("h3",{attrs:{id:"损失函数"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#损失函数"}},[t._v("#")]),t._v(" 损失函数")]),t._v(" "),a("p",[t._v("损失函数是表示神经网络性能好坏的指标，即当前的神经网络对监督数据在多大程度上不拟合，一般用"),a("strong",[t._v("均方误差")]),t._v("和"),a("strong",[t._v("交叉熵误差")]),t._v("等。")]),t._v(" "),a("p",[a("strong",[t._v("one-hot编码")]),t._v("：是一种将离散特征转换为二进制向量的编码方式，核心思想是为每一个唯一的离散值分配一个独立的维度，并用和1表示该特征是否属于某个类别。")]),t._v(" "),a("p",[t._v("假设存在一个离散特征的取值集合为 "),a("code",[t._v("{A, B, C, D}")]),t._v("，则每个取值的 one-hot 编码如下：")]),t._v(" "),a("ul",[a("li",[t._v("A → [1, 0, 0, 0]")]),t._v(" "),a("li",[t._v("B → [0, 1, 0, 0]")]),t._v(" "),a("li",[t._v("C → [0, 0, 1, 0]")]),t._v(" "),a("li",[t._v("D → [0, 0, 0, 1]")])]),t._v(" "),a("p",[a("strong",[t._v("均方误差")]),t._v("：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251111154759841.png",alt:"image-20251111154759841"}})]),t._v(" "),a("p",[a("strong",[t._v("交叉熵误差")]),t._v("：tk中只有正确解标签的索引为1，单个数据和N个数据的公式如下：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251111154744785.png",alt:"image-20251111154744785"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251111155412458.png",alt:"image-20251111155412458"}})]),t._v(" "),a("h3",{attrs:{id:"梯度下降法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#梯度下降法"}},[t._v("#")]),t._v(" 梯度下降法")]),t._v(" "),a("p",[t._v("神经网络一般用梯度下降法寻找损失函数的最小值。代码实现求梯度，等同于求多个偏导数，偏导数的计算可以用数值微分解决。")]),t._v(" "),a("p",[a("strong",[t._v("学习率")]),t._v("：梯度下降法中控制模型参数更新步长的超参数，决定了每次迭代时模型向最优解靠近的幅度。学习率的选择很重要，过大过小都不行。")]),t._v(" "),a("p",[t._v("神经网络的学习步骤：")]),t._v(" "),a("ul",[a("li",[t._v("步骤1：mini-batch 从训练数据中随机选出一部分数据，称为mini-batch，目标是减小损失函数的值")]),t._v(" "),a("li",[t._v("步骤2：计算梯度，表示损失函数减小最多的方向")]),t._v(" "),a("li",[t._v("步骤3：更新参数：将权重参数沿梯度方向进行微小更新")]),t._v(" "),a("li",[t._v("步骤4：重复步骤1、步骤2、步骤3")])]),t._v(" "),a("h2",{attrs:{id:"误差反向传播法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#误差反向传播法"}},[t._v("#")]),t._v(" 误差反向传播法")]),t._v(" "),a("p",[t._v("反向传播传递“局部倒数”。")]),t._v(" "),a("p",[t._v("计算图中途求得的导数的结果可以被共享，从而可以高效地计算多个导数。")]),t._v(" "),a("p",[t._v("链式法则是关于复合函数的导数的性质。")]),t._v(" "),a("h3",{attrs:{id:"反向传播"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#反向传播"}},[t._v("#")]),t._v(" 反向传播")]),t._v(" "),a("p",[t._v("反向传播：")]),t._v(" "),a("ul",[a("li",[t._v("加法节点的反向传播将上游的值传给下游，不需要正向传播的输入信号。")]),t._v(" "),a("li",[t._v("乘法的反向传播将上游的值乘以正向传播的翻转值传递给下游，需要正向传播的输入信号值。")])]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251112081402930.png",alt:"image-20251112081402930"}})]),t._v(" "),a("h3",{attrs:{id:"激活函数"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#激活函数"}},[t._v("#")]),t._v(" 激活函数")]),t._v(" "),a("p",[t._v("ReLU激活函数：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251112082410654.png",alt:"image-20251112082410654"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251112082446109.png",alt:"image-20251112082446109"}})]),t._v(" "),a("p",[t._v("SigMod激活函数：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251112082652506.png",alt:"image-20251112082652506"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251112082830770.png",alt:"image-20251112082830770"}})]),t._v(" "),a("h3",{attrs:{id:"affine-softmax层"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#affine-softmax层"}},[t._v("#")]),t._v(" Affine/Softmax层")]),t._v(" "),a("ul",[a("li",[t._v("Affine层：对数据进行线性变换+偏置，即仿射变换")]),t._v(" "),a("li",[t._v("ReLU（Rectified Linear Unit修正线性单元）层：引入非线性，增强网络表达能力")]),t._v(" "),a("li",[t._v("SoftMax层：将输出转换为概率分布，用于分类决策")])]),t._v(" "),a("p",[t._v("配合关系："),a("strong",[t._v("输入 → Affine 层（线性变换）→ ReLU 层（非线性激活）→ ...（多次重复）→ Affine 层（输出 logits）→ SoftMax 层（输出概率）→ 分类决策")])]),t._v(" "),a("h2",{attrs:{id:"与学习相关的技巧"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#与学习相关的技巧"}},[t._v("#")]),t._v(" 与学习相关的技巧")]),t._v(" "),a("h3",{attrs:{id:"参数的更新"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参数的更新"}},[t._v("#")]),t._v(" 参数的更新")]),t._v(" "),a("p",[t._v("SGD随机梯度下降法：如果函数形状呈延伸状，搜索路径就会比较低效")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251112161425606.png",alt:"image-20251112161425606"}})]),t._v(" "),a("p",[t._v("Momentum：考虑到速度，加速SGD")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251112161631268.png",alt:"image-20251112161631268"}})]),t._v(" "),a("p",[t._v("AdaGrad：为不同的参数自适应调整学习率，会记录过去所有梯度的平方和。（RMSProp方法会遗忘过去的参数，窗口滑动平均）")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251112161947177.png",alt:"image-20251112161947177"}})]),t._v(" "),a("p",[t._v("Adam：融合了Momentum和AdaGrad方法，学习的更新程度被适当调整")]),t._v(" "),a("ol",[a("li",[t._v("计算参数梯度的"),a("strong",[t._v("一阶矩估计")]),t._v("（类似动量，积累梯度的指数移动平均，反映梯度的方向趋势）；")]),t._v(" "),a("li",[t._v("计算参数梯度的"),a("strong",[t._v("二阶矩估计")]),t._v("（类似 RMSprop，积累梯度平方的指数移动平均，反映梯度的尺度信息）；")]),t._v(" "),a("li",[t._v("对这两个矩估计进行"),a("strong",[t._v("偏差修正")]),t._v("（解决初始阶段估计偏差问题）；")]),t._v(" "),a("li",[t._v("基于修正后的矩估计，动态调整每个参数的学习率，实现自适应更新。")])]),t._v(" "),a("h3",{attrs:{id:"权重的初始值"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#权重的初始值"}},[t._v("#")]),t._v(" 权重的初始值")]),t._v(" "),a("p",[t._v("梯度消失：偏向0和1的数据分布会造成反向传播中的梯度值不断变小，最后消失。")]),t._v(" "),a("p",[t._v("表现力受限：激活值在分布上有所偏向")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251112211613982.png",alt:"image-20251112211613982"}})]),t._v(" "),a("h2",{attrs:{id:"卷积神经网络"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#卷积神经网络"}},[t._v("#")]),t._v(" 卷积神经网络")]),t._v(" "),a("h3",{attrs:{id:"整体结构"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#整体结构"}},[t._v("#")]),t._v(" 整体结构")]),t._v(" "),a("p",[t._v("卷积神经网络（Convolutional Neural Network,CNN）")]),t._v(" "),a("p",[t._v("全连接：相邻层的所有神经元都有连接")]),t._v(" "),a("p",[t._v("一个5层的全连接的神经网络结构如下图：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251112212811051.png",alt:"image-20251112212811051"}})]),t._v(" "),a("p",[t._v("使用CNN会是什么样子呢？")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251112212835916.png",alt:"image-20251112212835916"}})]),t._v(" "),a("p",[t._v("Affine-ReLU层被替换为Convolution-RElu-(Pooling)")]),t._v(" "),a("h3",{attrs:{id:"卷积层"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#卷积层"}},[t._v("#")]),t._v(" 卷积层")]),t._v(" "),a("p",[t._v("全连接层忽视了数据的形状，例如图像是包含长、宽、通道的三维数据")]),t._v(" "),a("p",[t._v("卷积层的输入输出称为"),a("strong",[t._v("特征图")]),t._v("，卷积层的输入数据称为"),a("strong",[t._v("输入特征图")]),t._v("，卷积层的输出数据称为"),a("strong",[t._v("输出特征图")])]),t._v(" "),a("p",[t._v("CNN中，滤波器的参数对应之前的权重")]),t._v(" "),a("p",[t._v("填充：在进行卷积层的处理之前，有时要向输入数据的周围填充固定的数据，比如0")]),t._v(" "),a("p",[t._v("步幅：应用滤波器的位置间隔")]),t._v(" "),a("p",[t._v("输出大小的计算：输入大小为(H,W),滤波器大小为(FH,FW),输出大小为（OH,OW），填充为P，步幅为S")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251112214046970.png",alt:"image-20251112214046970"}})]),t._v(" "),a("p",[t._v("3维数据的卷积运算：通道方向上有多个特征图是，会按通道进行数据和滤波器的卷积运算，将结果叠加输出")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251112214345432.png",alt:"image-20251112214345432"}})]),t._v(" "),a("p",[t._v("批处理：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251112220906472.png",alt:"image-20251112220906472"}})]),t._v(" "),a("h3",{attrs:{id:"池化层"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#池化层"}},[t._v("#")]),t._v(" 池化层")]),t._v(" "),a("p",[t._v("池化是缩小高、长方向上的空间的运算。")]),t._v(" "),a("p",[t._v("池化层没有要学习的参数，通道数不发生变化，池化操作是按通道独立进行的。对微小的位置变化具有稳健性。")]),t._v(" "),a("p",[t._v("CNN各层间传递的是4维数据。")]),t._v(" "),a("p",[t._v("im2col函数：将卷积操作转化为矩阵乘法")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251112220446492.png",alt:"image-20251112220446492"}})]),t._v(" "),a("p",[t._v("左侧矩阵（A×B），A是有多少个卷积窗口，B是一个卷积窗口内有多少个元素（可扩展到N个数据）")]),t._v(" "),a("p",[t._v("右侧矩阵（C×D），C是一个卷积核内有多少个元素，D是卷积核的数量")]),t._v(" "),a("p",[t._v("输出数据（A×D），每一列可复原为一个二维矩阵")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251112221044320.png",alt:"image-20251112221044320"}})]),t._v(" "),a("h3",{attrs:{id:"cnn的可视化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cnn的可视化"}},[t._v("#")]),t._v(" CNN的可视化")]),t._v(" "),a("p",[t._v("CNN学习前和学习后卷积核的比较")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251112224000481.png",alt:"image-20251112224000481"}})]),t._v(" "),a("p",[t._v("AlexNet")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251112224249123.png",alt:"image-20251112224249123"}})]),t._v(" "),a("h2",{attrs:{id:"深度学习"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#深度学习"}},[t._v("#")]),t._v(" 深度学习")]),t._v(" "),a("p",[t._v("深度学习是加深了层的深度神经网络")]),t._v(" "),a("h3",{attrs:{id:"加深网络"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#加深网络"}},[t._v("#")]),t._v(" 加深网络")]),t._v(" "),a("p",[t._v("集成学习、学习率衰减、Data Augmentation（数据扩充）等都有助于提高识别精度。")]),t._v(" "),a("p",[t._v("数据扩充：人为的通过旋转、平移等微小变化，增加图像数量")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251112225131311.png",alt:"image-20251112225131311"}})]),t._v(" "),a("p",[t._v("加深层的一个好处是可以减少网络参数的数量")]),t._v(" "),a("h3",{attrs:{id:"加深学习的小历史"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#加深学习的小历史"}},[t._v("#")]),t._v(" 加深学习的小历史")]),t._v(" "),a("p",[t._v("ImageNet是拥有超过100万张图像的数据集，并且每张图像都被关联了标签。每年都会举办使用这个巨大数据集的ILSCRC图像识别大赛。")]),t._v(" "),a("p",[t._v("VG、GoogleNet、ResNet")]),t._v(" "),a("h3",{attrs:{id:"深度学习的高速化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#深度学习的高速化"}},[t._v("#")]),t._v(" 深度学习的高速化")]),t._v(" "),a("p",[t._v("GPU不仅用于图像处理，也用于通用的数值计算。深度学习中需要进行大量的乘积累加运算，或者大型矩阵的乘积运算，对这种大量的并行运算正式GPU所擅长的。")]),t._v(" "),a("p",[t._v("GPU主要有NVIDIA和AMD两家公司提供，但是大多数深度学习框架只受益于NNVIDIA的GPU，因为深度学习框架中使用了NVIDIA提供的CUDA这个面向GPU计算的综合开发环境。")]),t._v(" "),a("p",[t._v("分布式学习框架：Google的TensorFlow、微软的CNTK")]),t._v(" "),a("p",[t._v("深度学习中，16位的半精度浮点数，也可以顺利进行学习。")]),t._v(" "),a("h3",{attrs:{id:"深度学习的应用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#深度学习的应用"}},[t._v("#")]),t._v(" 深度学习的应用")]),t._v(" "),a("p",[t._v("物体的检测：从图像中确定物体位置，并进行分类")]),t._v(" "),a("p",[t._v("图像分割：在像素水平对图像进行分类")]),t._v(" "),a("p",[t._v("图像标题的生成")]),t._v(" "),a("p",[t._v("RNN（Recurrent Neural Network）是呈递归式连接的网络，经常被用于自然语言、时间序列数据的连续性数据上。NIC组合了CNN和RNN，可以生成惊人的高精度图像标题。")]),t._v(" "),a("h3",{attrs:{id:"深度学习的未来"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#深度学习的未来"}},[t._v("#")]),t._v(" 深度学习的未来")]),t._v(" "),a("p",[t._v("图像风格变换：风格迁移")]),t._v(" "),a("p",[t._v("图像生成：DCGAN（ Deep Convolutional Generative Adversarial Networks）深度卷积生成对抗网络、GAN（Generative Adversarial Networks）生成对抗网络")]),t._v(" "),a("p",[t._v("DCGAN使用了深度学习，技术要点是使用了生成者Generatpr生成近似真品的图像，使用识别者Discriminator判别是不是真图像。两者以竞争方式学习。")]),t._v(" "),a("p",[t._v("监督学习与无监督学习，数据集中的标签就属于监督数据。")]),t._v(" "),a("p",[t._v("自动驾驶")]),t._v(" "),a("p",[t._v("强化学习：代理（Agent）根据环境选择行动，然后通过行动改变环境。")]),t._v(" "),a("h2",{attrs:{id:"报错解决"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#报错解决"}},[t._v("#")]),t._v(" 报错解决")]),t._v(" "),a("h3",{attrs:{id:"pycharm中使用matplotlib可视化数据时"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pycharm中使用matplotlib可视化数据时"}},[t._v("#")]),t._v(" Pycharm中使用matplotlib可视化数据时")]),t._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("AttributeError: 'FigureCanvasInterAgg' object has no attribute 'tostring_rgb'. Did you mean: 'tostring_argb'?\n")])])]),a("p",[t._v("在Pycharm设置中的Tools->Python Plots取消下面两个选项就可解决")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251111170740768.png",alt:"image-20251111170740768"}})]),t._v(" "),a("p",[t._v("目前学习到240页")])])}),[],!1,null,null,null);a.default=_.exports}}]);