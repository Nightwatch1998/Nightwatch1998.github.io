(window.webpackJsonp=window.webpackJsonp||[]).push([[32],{316:function(t,a,s){"use strict";s.r(a);var v=s(10),_=Object(v.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"机器学习课程-李宏毅"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#机器学习课程-李宏毅"}},[t._v("#")]),t._v(" 机器学习课程-李宏毅")]),t._v(" "),a("p",[a("a",{attrs:{href:"https://www.bilibili.com/video/BV1Wv411h7kN?spm_id_from=333.788.videopod.episodes&vd_source=ed6f8916d713e6a2c8059c684a250ee6",target:"_blank",rel:"noopener noreferrer"}},[t._v("B站课程链接"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("a",{attrs:{href:"https://www.youtube.com/watch?v=7XZR0-4uS5s&list=PLJV_el3uVTsPM2mM-OQzJXziCGJa8nJL8",target:"_blank",rel:"noopener noreferrer"}},[t._v("youtube课程地址"),a("OutboundLink")],1)]),t._v(" "),a("h2",{attrs:{id:"机器学习的基本概念"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#机器学习的基本概念"}},[t._v("#")]),t._v(" 机器学习的基本概念")]),t._v(" "),a("p",[t._v("1-5 Supervised Learning")]),t._v(" "),a("p",[t._v("6 生成式对抗网络  Generative Adversarial Network")]),t._v(" "),a("p",[t._v("7 自监督学习 self-supervised Learning")]),t._v(" "),a("p",[t._v("BERT  基础模型")]),t._v(" "),a("p",[t._v("8 异常检测 Anomaly Detection")]),t._v(" "),a("p",[t._v("9 可解释性AI  Explainable AI")]),t._v(" "),a("p",[t._v("10 模型攻击 Model Attack")]),t._v(" "),a("p",[t._v("11 领域适应 Domain Adaptation")]),t._v(" "),a("p",[t._v("12 强化学习 Reinforcement Learning")]),t._v(" "),a("p",[t._v("13 模型压缩 Network Compression")]),t._v(" "),a("p",[t._v("14 终身机器学习 Life-long Learning")]),t._v(" "),a("p",[t._v("15 元学习-学习如何学习 Meta learning")]),t._v(" "),a("p",[t._v("google colab")]),t._v(" "),a("p",[t._v("浅谈机器学习原理")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251215221112172.png",alt:"image-20251215221112172"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251215222251713.png",alt:"image-20251215222251713"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251215223140651.png",alt:"image-20251215223140651"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251215223742080.png",alt:"image-20251215223742080"}})]),t._v(" "),a("p",[t._v("VC-dimension 描述一个模型复杂程度的指标")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/image-20251215224734087.png",alt:"image-20251215224734087"}})]),t._v(" "),a("p",[a("a",{attrs:{href:"https://www.youtube.com/watch?v=xQXh3fSvD1A&list=PLJV_el3uVTsPM2mM-OQzJXziCGJa8nJL8&index=3",target:"_blank",rel:"noopener noreferrer"}},[t._v("视频进度"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("structure learning 让机器产生有结构的输出")]),t._v(" "),a("p",[t._v("机器学习公式推导比较难的一步是计算梯度， 而pytorch框架可以自动计算梯度。")]),t._v(" "),a("h2",{attrs:{id:"机器学习任务攻略"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#机器学习任务攻略"}},[t._v("#")]),t._v(" 机器学习任务攻略")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224094027420.png",alt:"image-20251224094027420"}}),t._v(" "),a("p",[t._v("当LOSS比较大的时候，可能是模型偏差，也可能是优化没做好，怎么判断呢？")]),t._v(" "),a("ul",[a("li",[t._v("使用容易优化的浅层的网络，或者其他的模型")]),t._v(" "),a("li",[t._v("如果更深的模型在训练数据上没有获得更小的loss，那就是优化没做好")])]),t._v(" "),a("blockquote",[a("p",[t._v("深层网络一定可以做到浅层网络做到的事情，比如50层的网络，前20层可以和20层的网络做相同的事情，后30层什么也不做，那50层的网络效果和20层的网络就是一样的。")])]),t._v(" "),a("h3",{attrs:{id:"过拟合问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#过拟合问题"}},[t._v("#")]),t._v(" 过拟合问题")]),t._v(" "),a("p",[t._v("过拟合问题解决方案")]),t._v(" "),a("ul",[a("li",[t._v("增加训练数据")]),t._v(" "),a("li",[t._v("数据增强：比如图像领域机器学习，可以把图片左右翻转，部分放大")]),t._v(" "),a("li",[t._v("给模型一些限制：少的参数，共用参数")]),t._v(" "),a("li",[t._v("更少的要素")]),t._v(" "),a("li",[t._v("正则化")]),t._v(" "),a("li",[t._v("Early stopping：在验证集性能达到最优时，提前停止训练")]),t._v(" "),a("li",[t._v("Dropout")])]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224102254368.png",alt:"image-20251224102254368"}}),t._v(" "),a("p",[t._v("模型太简单会有模型偏差，模型过于复杂会过拟合。")]),t._v(" "),a("h3",{attrs:{id:"模型泛化能力"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#模型泛化能力"}},[t._v("#")]),t._v(" 模型泛化能力")]),t._v(" "),a("p",[t._v("公共测试集：比赛开发阶段用于评估模型性能的测试数据")]),t._v(" "),a("p",[t._v("私有测试集：比赛最终阶段（盲测阶段）使用的隐藏测试数据")]),t._v(" "),a("p",[t._v("因此需要把训练数据集分为两部分，一部分用于训练，一部分用于验证。选择验证集上LOSS最小的模型就好。")]),t._v(" "),a("h3",{attrs:{id:"n-折交叉验证"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#n-折交叉验证"}},[t._v("#")]),t._v(" n 折交叉验证")]),t._v(" "),a("p",[t._v("用于评估模型的泛化能力")]),t._v(" "),a("p",[t._v("将"),a("strong",[t._v("整个数据集")]),t._v("随机且均匀地划分为 "),a("code",[t._v("n")]),t._v(" 个互斥的子集（称为 "),a("strong",[t._v("fold")]),t._v("，即 “折”），然后重复 "),a("code",[t._v("n")]),t._v(" 次训练和验证：")]),t._v(" "),a("ol",[a("li",[t._v("每次选取 "),a("strong",[t._v("1 个 fold 作为验证集")]),t._v("，剩下的 "),a("strong",[t._v("n-1 个 fold 作为训练集")]),t._v("；")]),t._v(" "),a("li",[t._v("用训练集训练模型，用验证集计算性能指标（如准确率、Loss）；")]),t._v(" "),a("li",[t._v("完成 "),a("code",[t._v("n")]),t._v(" 次循环后，取 "),a("strong",[t._v("n 次验证指标的平均值")]),t._v("，作为模型的最终评估结果。")])]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224104702357.png",alt:"image-20251224104702357"}}),t._v(" "),a("p",[t._v("mismatch：分布偏移或模型不匹配。模型的训练逻辑是架设训练数据和测试数据服从相同的概率分布，一旦这个架设不成立，模型学到的规律就无法适配测试数据。")]),t._v(" "),a("h3",{attrs:{id:"鱼和熊掌可以兼得"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#鱼和熊掌可以兼得"}},[t._v("#")]),t._v(" 鱼和熊掌可以兼得")]),t._v(" "),a("p",[t._v("传统的机器学习中，模型越复杂，越容易过拟合，模型越简单，泛化能力越强，但无法处理复杂任务。")]),t._v(" "),a("p",[t._v("深度学习可以同时兼顾"),a("strong",[t._v("高模型容量")]),t._v("（捕捉复杂模式的能力）和"),a("strong",[t._v("强泛化能力")]),t._v("（适配新数据的能力）")]),t._v(" "),a("h2",{attrs:{id:"模型优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#模型优化"}},[t._v("#")]),t._v(" 模型优化")]),t._v(" "),a("h3",{attrs:{id:"鞍点、局部最小、局部最大"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#鞍点、局部最小、局部最大"}},[t._v("#")]),t._v(" 鞍点、局部最小、局部最大")]),t._v(" "),a("p",[t._v("当梯度为0时，不一定是局部最小值，也有可能是鞍点。")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224105658657.png",alt:"image-20251224105658657"}}),t._v(" "),a("p",[t._v("用泰勒展开计算：")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224110411782.png",alt:"image-20251224110411782"}}),t._v(" "),a("p",[t._v("H是海森矩阵，多元函数的的二阶偏导数方阵，可以使用海森矩阵正负定性判断极值点。")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224110957649.png",alt:"image-20251224110957649"}}),t._v(" "),a("p",[t._v("正定矩阵：实对称矩阵所有特征值大于0，称为正定矩阵。")]),t._v(" "),a("blockquote",[a("p",[t._v("海森矩阵的特征值有正有负，表示该点是鞍点。")]),t._v(" "),a("p",[t._v("沿着"),a("code",[t._v("负特征值")]),t._v("对应的"),a("code",[t._v("特征向量")]),t._v("方向更新参数，可以降低LOSS。")])]),t._v(" "),a("p",[t._v("二维的局部最小值在三维看来可能是一个鞍点，低维度看起来无路可走，在高维度其实是有路的。")]),t._v(" "),a("h3",{attrs:{id:"批次和动量"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#批次和动量"}},[t._v("#")]),t._v(" 批次和动量")]),t._v(" "),a("p",[t._v("为什么要用batch?")]),t._v(" "),a("p",[t._v("当并行计算时，较大的batch size计算梯度时不一定比小的batch size 花的时间长。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224142331956.png",alt:"image-20251224142331956"}})]),t._v(" "),a("p",[t._v("小的batch size 在优化和测试集上效果更好。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224143235938.png",alt:"image-20251224143235938"}})]),t._v(" "),a("p",[a("strong",[t._v("动量Momentum")])]),t._v(" "),a("p",[t._v("梯度下降法：每次朝着梯度的反方向更新一步。")]),t._v(" "),a("p",[t._v("然而，在物理世界中，小球沿着Loss函数滚落，由于带有速度，不会在局部最小值停下。")]),t._v(" "),a("blockquote",[a("p",[t._v("动量只是类比物理世界中物体运动惯性的特点。")])]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224143520685.png",alt:"image-20251224143520685"}}),t._v(" "),a("p",[t._v("考虑动量的计算：每一步的变化量=上一步的变化量-当前步的梯度")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224144713982.png",alt:"image-20251224144713982"}}),t._v(" "),a("blockquote",[a("p",[t._v("引入动量有什么好处？")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("加速收敛速度")])]),t._v(" "),a("li",[a("strong",[t._v("抑制参数更新震荡")])]),t._v(" "),a("li",[a("strong",[t._v("突破局部极小值或鞍点")])])])]),t._v(" "),a("h3",{attrs:{id:"自动调整学习率"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#自动调整学习率"}},[t._v("#")]),t._v(" 自动调整学习率")]),t._v(" "),a("p",[t._v("当Loss不再减小的时候，并不意味着梯度一定很小，有可能在Minima附近震荡。")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224150109081.png",alt:"image-20251224150109081"}}),t._v(" "),a("blockquote",[a("p",[t._v("为什么需要自适应学习率？")]),t._v(" "),a("p",[t._v("固定学习率无法适配复杂误差曲面的不同区域，容易导致收敛慢、震荡或不收敛。")]),t._v(" "),a("p",[a("strong",[t._v("哪里难走，就放慢脚步；哪里好走，就加快速度")])])]),t._v(" "),a("h4",{attrs:{id:"典型的自适应算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#典型的自适应算法"}},[t._v("#")]),t._v(" 典型的自适应算法")]),t._v(" "),a("p",[t._v("Adagrad算法：累积参数的历史梯度平方和")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224152651590.png",alt:"image-20251224152651590"}}),t._v(" "),a("p",[t._v("RMSprop：指数移动平均累积梯度平方和")]),t._v(" "),a("blockquote",[a("p",[t._v("衡量近期梯度的波动幅度，远期的梯度影响会指数级衰减。")]),t._v(" "),a("p",[t._v("衰减系数α通常取0.9")])]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224153424585.png",alt:"image-20251224153424585"}}),t._v(" "),a("p",[a("strong",[t._v("Adam：结合动量 + RMSprop 的思想")])]),t._v(" "),a("h4",{attrs:{id:"学习率调度"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#学习率调度"}},[t._v("#")]),t._v(" 学习率调度")]),t._v(" "),a("p",[t._v("Learning Rate Decay：学习率衰减")]),t._v(" "),a("p",[t._v("初期学习率较大，末期学习率较小。")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224155358715.png",alt:"image-20251224155358715"}}),t._v(" "),a("p",[t._v("Warm Up：学习率先变大、后变小")]),t._v(" "),a("blockquote",[a("p",[t._v("在很多知名的网络都有使用。")])]),t._v(" "),a("h3",{attrs:{id:"不同的损失函数loss"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#不同的损失函数loss"}},[t._v("#")]),t._v(" 不同的损失函数Loss")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224162208465.png",alt:"image-20251224162208465"}})]),t._v(" "),a("p",[t._v("交叉熵最小等价于最大似然")]),t._v(" "),a("p",[a("strong",[t._v("交叉熵通常用于分类任务")]),t._v("。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20251224163754523.png",alt:"image-20251224163754523"}})]),t._v(" "),a("h2",{attrs:{id:"卷积神经网络cnn"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#卷积神经网络cnn"}},[t._v("#")]),t._v(" 卷积神经网络CNN")]),t._v(" "),a("p",[t._v("图像识别的卷积核一般是3×3, 加深层数后仍然可以识别更大形状的pattern")]),t._v(" "),a("p",[t._v("图像是一个三维的tensor，三个维度分别是图像的宽、高、通道数")]),t._v(" "),a("p",[t._v("过滤器：用相同的一组卷积核扫过整个图像，这个卷积核可以认为是一个过滤器。每一层的过滤器可以有很多个。对应下一层图像的通道数。")]),t._v(" "),a("p",[t._v("池化操作：在保留核心特征的情况下，对特征图进行降维（减小尺寸）。(最大池化、平均池化)")]),t._v(" "),a("blockquote",[a("p",[t._v("原始图像-卷积-池化-卷积-池化-flatten-全连接层-softmax")]),t._v(" "),a("p",[t._v("如果算力足够，也可以不用做池化")])]),t._v(" "),a("blockquote",[a("p",[t._v("案例：下围棋其实是一个分类问题，可以用CNN解决，19×19的棋盘，通道就是该位置的状态。")]),t._v(" "),a("p",[t._v("Alfgo使用的网络架构：19×19×4，卷积核是5×5，边缘补0，步长为1,192个filter，没有用池化。")])]),t._v(" "),a("p",[t._v("CNN不能处理影响放大、缩小、旋转的问题。")]),t._v(" "),a("blockquote",[a("p",[t._v("Special Tranformer Layer可以处理放大、缩小、旋转的问题")])]),t._v(" "),a("p",[t._v("今年来，CNN也用在语音和图像上，但是网络设计、卷积核设计是不一样的。")]),t._v(" "),a("h2",{attrs:{id:"注意力机制"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#注意力机制"}},[t._v("#")]),t._v(" 注意力机制")]),t._v(" "),a("h3",{attrs:{id:"sequence-labeling"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sequence-labeling"}},[t._v("#")]),t._v(" Sequence Labeling")]),t._v(" "),a("p",[t._v("输入是一堆向量，而且长度不一，通过word embedding给每个词汇一个向量。例如：")]),t._v(" "),a("ul",[a("li",[t._v("文字处理")]),t._v(" "),a("li",[t._v("语音识别")]),t._v(" "),a("li",[t._v("分子结构式")])]),t._v(" "),a("p",[t._v("输出类型：")]),t._v(" "),a("ul",[a("li",[t._v("每个向量输出一个label")]),t._v(" "),a("li",[t._v("整个序列输出一个label")]),t._v(" "),a("li",[t._v("机器自己决定输出几个label（"),a("strong",[t._v("seq2seq")]),t._v("）")])]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20260113094233148.png",alt:"image-20260113094233148"}}),t._v(" "),a("p",[t._v("考虑上下文")]),t._v(" "),a("h3",{attrs:{id:"self-attention"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#self-attention"}},[t._v("#")]),t._v(" Self-attention")]),t._v(" "),a("p",[t._v("每个输出向量都考虑一整个sequence的信息。可以作为层，可叠加。")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20260113094921257.png",alt:"image-20260113094921257"}}),t._v(" "),a("p",[t._v("如何计算Self-attention的输出元素b1?")]),t._v(" "),a("blockquote",[a("p",[t._v("计算输入a1和其他输入元素的相关性。")])]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20260113095813908.png",alt:"image-20260113095813908"}}),t._v(" "),a("p",[t._v("如何计算相关性α呢？")]),t._v(" "),a("blockquote",[a("p",[t._v("计算输入a1的q1矩阵和a1-a4对应的k矩阵的点乘，得到a1和a1-a4的相关性，然后计算softmax。")]),t._v(" "),a("p",[t._v("于是知道了哪些向量和a1是最有关系的。")])]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20260113100313760.png",alt:"image-20260113100313760"}}),t._v(" "),a("blockquote",[a("p",[t._v("a1-a4每个向量都乘以W得到对应的v，将每个v乘以α后求和得到b1。")]),t._v(" "),a("p",[a("strong",[t._v("b1-b4可以并行计算")])])]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20260113100707725.png",alt:"image-20260113100707725"}}),t._v(" "),a("h3",{attrs:{id:"矩阵乘法的角度理解"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#矩阵乘法的角度理解"}},[t._v("#")]),t._v(" 矩阵乘法的角度理解")]),t._v(" "),a("p",[t._v("把输入的a1-a4分别乘以Wq、Wk、Wv，")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20260113102052635.png",alt:"image-20260113102052635"}}),t._v(" "),a("p",[t._v("每一个α，都由对应的q和k的转置相乘得到，将k的列向量组合后转置，乘以q的列向量组合，得到相关性a的矩阵，随后进行正规化。")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20260113102904656.png",alt:"image-20260113102904656"}}),t._v(" "),a("p",[t._v("b1就相当于v的列向量组成的矩阵左乘α1的列向量，将b1-b4组合起来就得到self-attention的输出")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20260113103615551.png",alt:"image-20260113103615551"}}),t._v(" "),a("p",[t._v("self-atention中需要学习的参数，就只有Wq、Wk、Wv")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20260113104119463.png",alt:"image-20260113104119463"}}),t._v(" "),a("h3",{attrs:{id:"多头注意力机制"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#多头注意力机制"}},[t._v("#")]),t._v(" 多头注意力机制")]),t._v(" "),a("p",[t._v("相关性的定义可以有很多种不同的形式，a和b可以在不同层面都具有相关性。")]),t._v(" "),a("blockquote",[a("p",[t._v("比如说苹果和樱桃，都是水果，也都是红色的")])]),t._v(" "),a("p",[t._v("所谓的多头注意力，就是每个输入a都有多组对应的qkv，每一组计算得到b后，再乘以一个矩阵得到输出结果。")]),t._v(" "),a("p",[t._v("多组的qkv，由原始的qkv分别乘以一个矩阵得到。")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20260113105433592.png",alt:"image-20260113105433592"}}),t._v(" "),a("p",[t._v("自注意力机制没有位置信息")]),t._v(" "),a("p",[t._v("positional encoding")]),t._v(" "),a("h3",{attrs:{id:"应用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#应用"}},[t._v("#")]),t._v(" 应用")]),t._v(" "),a("p",[t._v("self-attention被广泛用于NLP，例如transformer、Bert")]),t._v(" "),a("p",[t._v("其他的领域：语音识别，影像处理")]),t._v(" "),a("p",[t._v("Self-attention vs  CNN")]),t._v(" "),a("blockquote",[a("p",[t._v("CNN是简化版的self-attention。")]),t._v(" "),a("p",[t._v("self-attention弹性比较大，训练资料比较少的时候，容易过拟合")]),t._v(" "),a("p",[t._v("CNN弹性比较小，训练资料少的时候效果比较好")])]),t._v(" "),a("p",[t._v("Self-attention vs  RNN")]),t._v(" "),a("blockquote",[a("p",[t._v("RNN序列末端的输出较难考虑到序列最开始的输入，除非将最开始的输入信息保存，传递下去。")]),t._v(" "),a("p",[t._v("SA没有这个问题，不管序列多长，只要有相关性，就能关联到。")]),t._v(" "),a("p",[t._v("RNN没法并行处理，SA可以并行处理。")])]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20260113141410200.png",alt:"image-20260113141410200"}}),t._v(" "),a("p",[t._v("Self-attention for Graph：GNN（Graph Neural Networrk）")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20260113141839791.png",alt:"image-20260113141839791"}}),t._v(" "),a("p",[t._v("Self-attention的变形后来都命名为XXformer")]),t._v(" "),a("h2",{attrs:{id:"batch-normalization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#batch-normalization"}},[t._v("#")]),t._v(" Batch Normalization")]),t._v(" "),a("p",[t._v("为什么要做数据标准化？")]),t._v(" "),a("blockquote",[a("p",[t._v("核心原因是数据的不同维度有不同的数值范围，对相同的权重变化量贡献差异较大，导致在不同维度上Loss的收敛速度不同。")]),t._v(" "),a("p",[t._v("数据标准化就是把一组数据限制在0-1之间，同时保留数据的分布特征。")])]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20260113153957633.png",alt:"image-20260113153957633"}}),t._v(" "),a("p",[t._v("数据结果z同样做标准化，这样做之后，a1 a2 a3 就产生了相关性，改变z1，就会改变"),a("strong",[t._v("μ")]),t._v("和"),a("strong",[t._v("σ")]),t._v("，从而改变a1 a2 a3的值。")]),t._v(" "),a("img",{staticStyle:{zoom:"80%"},attrs:{src:"https://stag-blog.oss-cn-beijing.aliyuncs.com/picture/image-20260113154516253.png",alt:"image-20260113154516253"}}),t._v(" "),a("p",[t._v("为什么要做批处理？")]),t._v(" "),a("blockquote",[a("p",[t._v("因为数据了太大了，无法一次性把所有数据放到内存里。")])]),t._v(" "),a("p",[t._v("测试环节通常要对每一笔资料都产生一个输出，而不是等到攒够一个batch，但是没有一个batch就没有"),a("strong",[t._v("μ")]),t._v("和"),a("strong",[t._v("σ")]),t._v("，pytorch会在训练的时候通过移动平均自动帮我们计算。")]),t._v(" "),a("h2",{attrs:{id:"问题总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#问题总结"}},[t._v("#")]),t._v(" 问题总结")]),t._v(" "),a("h3",{attrs:{id:"为什么用了验证集-结果还是过拟合呢"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#为什么用了验证集-结果还是过拟合呢"}},[t._v("#")]),t._v(" 为什么用了验证集，结果还是过拟合呢？")]),t._v(" "),a("p",[t._v("其实用验证集来挑模型的过程，也可以看做是在训练")]),t._v(" "),a("h2",{attrs:{id:"名词术语"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#名词术语"}},[t._v("#")]),t._v(" 名词术语")]),t._v(" "),a("p",[t._v("Model Bias：模型偏差，衡量模拟拟合能力不足的指标")]),t._v(" "),a("p",[t._v("piecewise linear ：分段线性")]),t._v(" "),a("p",[t._v("Loss：损失函数 means how good a et of values is .")]),t._v(" "),a("p",[t._v("epoch：计算梯度下降时，当所有的batch都计算一遍，作为一个epoch")]),t._v(" "),a("p",[t._v("update：每一次计算batch，更新参数，叫做update")]),t._v(" "),a("p",[t._v("hyperparameter： 超参数，机器学习中非机器自动计算的，人为设定的参数，比如batchsize，学习率等等")]),t._v(" "),a("p",[t._v("sigmoid函数")]),t._v(" "),a("p",[t._v("ReLU函数(Rectified Linear Unit)：修正线性单元")]),t._v(" "),a("p",[t._v("Nerual Network：神经网络")]),t._v(" "),a("p",[t._v("Overfitting：过拟合，在训练资料上变好，在测试资料上变差的情况")]),t._v(" "),a("p",[t._v("benchmark corpora：基准语料库，是自然语言处理（NLP）领域中，用于评估模型性能、对比不同算法效果的标准化数据集。")]),t._v(" "),a("p",[t._v("critical point ：临界点")]),t._v(" "),a("p",[t._v("error surface：误差曲面，损失函数关于参数的多维曲面。")])])}),[],!1,null,null,null);a.default=_.exports}}]);